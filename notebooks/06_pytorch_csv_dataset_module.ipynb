{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from argparse import ArgumentParser, Namespace\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import pickle\n",
    "from typing import Tuple, List, Dict, Union, Optional, TypeVar, Type\n",
    "from itertools import permutations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.engineer.geowiki import GeoWikiDataInstance, GeoWikiEngineer\n",
    "from src.exporters.sentinel.cloudfree import BANDS\n",
    "from src.models import STR2MODEL, train_model\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create custom Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "GeowikiDatasetType = TypeVar('GeowikiDatasetType', bound='Parent') # for typing\n",
    "\n",
    "class GeowikiDataset(Dataset):\n",
    "    \n",
    "    dataset_name: str = GeoWikiEngineer.dataset\n",
    "    csv_file: str = 'geowiki_labels_country_crs4326.csv'\n",
    "\n",
    "    def __init__(self, data_folder: Path = Path('../data'),\n",
    "                countries_subset: Optional[List[str]] = None,\n",
    "                countries_to_weight: Optional[List[str]] = None,\n",
    "                crop_probability_threshold: float = 0.5,\n",
    "                remove_b1_b10: bool = True,\n",
    "                normalizing_dict: Optional[Dict] = None,\n",
    "                labels: Optional[pd.DataFrame] = None # if this is passed csv_file will be ignored\n",
    "                ) -> None:\n",
    "        \n",
    "        # Attributes\n",
    "        self.data_folder = data_folder\n",
    "        self.dataset_dir = data_folder / \"features\" / self.dataset_name\n",
    "        self.countries_subset = countries_subset\n",
    "        self.countries_to_weight = countries_to_weight\n",
    "        self.bands_to_remove = [\"B1\", \"B10\"]\n",
    "        self.remove_b1_b10 = remove_b1_b10\n",
    "        self.crop_probability_threshold = crop_probability_threshold\n",
    "        \n",
    "        # Functions\n",
    "        if labels is None:\n",
    "            self.labels = pd.read_csv(self.dataset_dir / self.csv_file)\n",
    "            self.labels.loc[self.labels['country'].isnull(), 'country'] = 'unknown'\n",
    "            if self.countries_subset:\n",
    "                self.labels = self.labels[self.labels['country'].str.lower().isin(list(map(str.lower, self.countries_subset)))].reset_index(drop=True)\n",
    "        else:\n",
    "            self.labels = labels\n",
    "        self.pickle_files = self.get_pickle_files_paths(self.dataset_dir / 'all')\n",
    "        self.file_identifiers_countries_to_weight = self.get_file_ids_for_countries(self.countries_to_weight)\n",
    "        print('length labels:', len(self.labels))\n",
    "        print('length pickle files:', len(self.pickle_files))\n",
    "        print('length local ids:', len(self.file_identifiers_countries_to_weight))\n",
    "\n",
    "        # Normalizing dictionary\n",
    "        if normalizing_dict is None:\n",
    "            self.normalizing_dict = self.get_normalizing_dict()\n",
    "        else:\n",
    "            self.normalizing_dict = normalizing_dict    \n",
    "        print(self.normalizing_dict)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Return the data, label, and weight tensors.\n",
    "        \"\"\"\n",
    "        target_file = self.pickle_files[index]\n",
    "        identifier = int(target_file.name.split('_')[0])\n",
    "\n",
    "        with target_file.open(\"rb\") as f:\n",
    "            target_datainstance = pickle.load(f)\n",
    "\n",
    "        if isinstance(target_datainstance, GeoWikiDataInstance):\n",
    "            if self.crop_probability_threshold is None:\n",
    "                label = target_datainstance.crop_probability\n",
    "            else:\n",
    "                label = int(target_datainstance.crop_probability >= self.crop_probability_threshold)\n",
    "        else:\n",
    "            raise RuntimeError(f\"Unrecognized data instance type {type(target_datainstance)}\")\n",
    "\n",
    "        is_local = 0\n",
    "        if identifier in self.file_identifiers_countries_to_weight:\n",
    "            is_local = 1\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(\n",
    "                self.remove_bands(x=self._normalize(target_datainstance.labelled_array))\n",
    "            ).float(),\n",
    "            torch.tensor(label).float(),\n",
    "            torch.tensor(is_local).long(),\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def num_output_classes(self) -> int:\n",
    "        return 1\n",
    "\n",
    "    @property\n",
    "    def num_input_features(self) -> int:\n",
    "        # assumes the first value in the tuple is x\n",
    "        assert len(self.pickle_files) > 0, \"No files to load!\"\n",
    "        output_tuple = self[0]\n",
    "        return output_tuple[0].shape[1]\n",
    "\n",
    "    @property\n",
    "    def num_timesteps(self) -> int:\n",
    "        # assumes the first value in the tuple is x\n",
    "        assert len(self.pickle_files) > 0, \"No files to load!\"\n",
    "        output_tuple = self[0]\n",
    "        return output_tuple[0].shape[0]\n",
    "\n",
    "    def remove_bands(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Expects the input to be of shape [timesteps, bands]\n",
    "        \"\"\"\n",
    "        if self.remove_b1_b10:\n",
    "            indices_to_remove: List[int] = []\n",
    "            for band in self.bands_to_remove:\n",
    "                indices_to_remove.append(BANDS.index(band))\n",
    "\n",
    "            bands_index = 1 if len(x.shape) == 2 else 2\n",
    "            indices_to_keep = [i for i in range(x.shape[bands_index]) if i not in indices_to_remove]\n",
    "            if len(x.shape) == 2:\n",
    "                # timesteps, bands\n",
    "                return x[:, indices_to_keep]\n",
    "            else:\n",
    "                # batches, timesteps, bands\n",
    "                return x[:, :, indices_to_keep]\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def _normalize(self, array: np.ndarray) -> np.ndarray:\n",
    "        if self.normalizing_dict is None:\n",
    "            return array\n",
    "        else:\n",
    "            return (array - self.normalizing_dict[\"mean\"]) / self.normalizing_dict[\"std\"]\n",
    "\n",
    "    @staticmethod\n",
    "    def load_files_and_normalizing_dict(\n",
    "        features_dir: Path, subset_name: str='training', file_name: str=\"normalizing_dict.pkl\"\n",
    "    ) -> Tuple[List[Path], Optional[Dict[str, np.ndarray]]]:\n",
    "        pickle_files = list((features_dir / subset_name).glob(\"*.pkl\"))\n",
    "\n",
    "        # try loading the normalizing dict. By default, if it exists we will use it\n",
    "        if (features_dir / file_name).exists():\n",
    "            with (features_dir / file_name).open(\"rb\") as f:\n",
    "                normalizing_dict = pickle.load(f)\n",
    "        else:\n",
    "            normalizing_dict = None\n",
    "\n",
    "        return pickle_files, normalizing_dict\n",
    "\n",
    "    def search_normalizing_dict(self, default_file_name: str=\"normalizing_dict.pkl\") -> Optional[Path]:\n",
    "        '''\n",
    "        Searches for the normalizing dict file in the self.data_dir directory and returns its path. Returns None if it was not found.\n",
    "        '''\n",
    "        prefix = default_file_name.split('.')[0]\n",
    "        \n",
    "        if not self.countries_subset:\n",
    "            file_path = self.dataset_dir / default_file_name\n",
    "            if file_path.exists():\n",
    "                print(f'Found normalizing dict {file_path.name}')\n",
    "                return file_path\n",
    "        elif len(self.countries_subset) == 1 and self.countries[0].lower() == 'africa':\n",
    "            raise NotImplementedError # TODO\n",
    "\n",
    "        else:\n",
    "            assert len(self.countries_subset) < 10, 'Execution time will be too big!' # TODO: add warning when passing subset to constructor\n",
    "            countries_permutations = list(permutations(self.countries_subset))\n",
    "            countries_permutations = ['_'.join(permutation) for permutation in countries_permutations]\n",
    "            for permutation in countries_permutations:\n",
    "                file_name = f\"{prefix}_{permutation}.pkl\"\n",
    "                file_path = self.dataset_dir / file_name\n",
    "                if file_path.exists():\n",
    "                    print(f'Found normalizing dict {file_name}')\n",
    "                    return file_path\n",
    "        print('Normalizing dict not found.')\n",
    "        return None\n",
    "\n",
    "    def get_normalizing_dict(self, save: bool=False) -> Dict:\n",
    "        # Return dict if it was found or create it and save\n",
    "        default_file_name = \"normalizing_dict.pkl\"\n",
    "        file_path = self.search_normalizing_dict(default_file_name)\n",
    "        if file_path:\n",
    "            print('Loading normalizing dict.')\n",
    "            return self.load_files_and_normalizing_dict(self.dataset_dir, file_name=file_path.name)[1]\n",
    "        else:\n",
    "            print('Calculating normalizing dict...')\n",
    "            assert len(self) == len(self.pickle_files), 'Length of self.labels must be the same as of the list of pickle files.'\n",
    "            geowiki_engineer = GeoWikiEngineer(Path('../data'))\n",
    "            \n",
    "            for file_path in tqdm(self.pickle_files):\n",
    "                identifier = int(file_path.name.split('_')[0])\n",
    "                with file_path.open(\"rb\") as f:   \n",
    "                    target_datainstance = pickle.load(f)\n",
    "                geowiki_engineer.update_normalizing_values(target_datainstance.labelled_array)\n",
    "\n",
    "            normalizing_dict = geowiki_engineer.calculate_normalizing_dict()\n",
    "\n",
    "            # Write file\n",
    "            if save:    \n",
    "                if self.countries_subset:\n",
    "                    prefix = default_file_name.split('.')[0]\n",
    "                    countries_str = '_'.join(self.countries_subset)\n",
    "                    file_name = f\"{prefix}_{countries_str}.pkl\"\n",
    "                else:\n",
    "                    file_name = default_file_name\n",
    "                file_path = self.data_dir / file_name\n",
    "                print('Saving normalizing dict', file_path.name)\n",
    "                with file_path.open(\"wb\") as f:\n",
    "                    pickle.dump(normalizing_dict, f)\n",
    "\n",
    "            return normalizing_dict\n",
    "            \n",
    "    def get_pickle_files_paths(self, folder_path: Path) -> Tuple[List[Path]]:\n",
    "        file_paths = self.labels.filename.tolist()\n",
    "        print('Checking for data files')\n",
    "        pickle_files = [path for path in tqdm(folder_path.glob('*.pkl')) if path.name in file_paths]\n",
    "        self._check_label_files(pickle_files)\n",
    "        return pickle_files\n",
    "\n",
    "    def _check_label_files(self, pickle_files) -> None:\n",
    "        same_files = set([file.name for file in pickle_files]) == set(self.labels.filename.tolist())\n",
    "        assert same_files, \"Some pickle files of the labels were not found!\"\n",
    "        print('All pickle files were found!')\n",
    "\n",
    "    def get_file_ids_for_countries(self, countries_list: List[str]) -> List[int]:\n",
    "        file_ids = []\n",
    "        if countries_list:\n",
    "            countries_list_lowercase = list(map(str.lower, countries_list))\n",
    "            file_ids.extend(self.labels[self.labels['country'].str.lower().isin(countries_list_lowercase)]['identifier'].tolist())\n",
    "        return file_ids\n",
    "\n",
    "    @classmethod\n",
    "    def train_val_split(cls: Type[GeowikiDatasetType], class_instance: Type[GeowikiDatasetType],\n",
    "                        train_size: float=0.8, stratify_column: Optional[str]=None\n",
    "                        ) -> Tuple[GeowikiDatasetType]:\n",
    "        # Made it a class method to be able to generate two child instances of the class.\n",
    "        # Haven't figured out a better way for now than passing the parent instance as an argument.\n",
    "        \n",
    "        # Split labels dataframe\n",
    "        stratify = None if not stratify_column else class_instance.labels[stratify_column]\n",
    "        df_train, df_val = train_test_split(class_instance.labels, train_size=train_size, stratify=stratify, random_state=42)\n",
    "        df_train.reset_index(drop=True, inplace=True) \n",
    "        df_val.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # Create two new GeowikiDataset instances (train and val)\n",
    "        print('Train split')\n",
    "        train_dataset = cls(countries_to_weight=class_instance.countries_to_weight,\n",
    "                        normalizing_dict=class_instance.normalizing_dict, labels=df_train)\n",
    "        print('Val split')\n",
    "        val_dataset = cls(countries_to_weight=class_instance.countries_to_weight,\n",
    "                        normalizing_dict=class_instance.normalizing_dict, labels=df_val)\n",
    "        return train_dataset, val_dataset\n",
    "\n",
    "    def get_file_by_identifier(self):\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for data files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35599it [00:00, 93443.17it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All pickle files were found!\n",
      "length labels: 837\n",
      "length pickle files: 837\n",
      "length local ids: 490\n",
      "Found normalizing dict normalizing_dict_Nigeria_Cameroon_Benin_Togo_Ghana.pkl\n",
      "Loading normalizing dict.\n",
      "{'mean': array([0.17615442, 0.15639622, 0.15478129, 0.16052005, 0.18210957,\n",
      "       0.25861192, 0.30205652, 0.28616927, 0.3337731 , 0.07249666,\n",
      "       0.00730804, 0.29148372, 0.19433004, 0.3125438 ]), 'std': array([0.07856543, 0.08016264, 0.07900642, 0.09663687, 0.08940903,\n",
      "       0.08163167, 0.08889727, 0.08184323, 0.09073389, 0.05365633,\n",
      "       0.01807999, 0.10828142, 0.10158471, 0.19213894])}\n"
     ]
    }
   ],
   "source": [
    "subset = ['Ghana', 'Togo', 'Nigeria', 'Cameroon', 'Benin'] #['Nigeria'] #None\n",
    "#subset = ['Ghana', 'Togo', 'Nigeria', 'Chad', 'Democratic Republic of the Congo', 'Ethiopia', 'Chad', 'Mali']\n",
    "dataset = GeowikiDataset(countries_subset=subset, countries_to_weight=['Nigeria'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to get splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt by splitting Geowiki dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[670, 167]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ratio = 0.2\n",
    "test_size = math.floor(len(dataset) * test_ratio)\n",
    "lenghts = [len(dataset) - test_size, test_size]\n",
    "lenghts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "837"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(lenghts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = random_split(dataset, lenghts)  #generator is not yet available in this pytorch version, generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.pickle_files # Subset class doesn't inheret properties like pickles files plus I would need to subset those as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With sklearn train test split on dataframe with labels and then create separate Geowiki datasets for each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset.labels\n",
    "df_subset = df[df['country'].isin(['Nigeria', 'Ghana'])]\n",
    "df_subset.groupby('country').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df_subset, test_size=0.1, stratify=df_subset['country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.groupby('country').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.groupby('country').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df_subset, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.groupby('country').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.groupby('country').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['country'].value_counts().to_csv('geowiki_points_per_country.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['country'].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split\n",
      "Checking for data files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35599it [00:00, 106750.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All pickle files were found!\n",
      "length labels: 669\n",
      "length pickle files: 669\n",
      "length local ids: 388\n",
      "{'mean': array([0.17615442, 0.15639622, 0.15478129, 0.16052005, 0.18210957,\n",
      "       0.25861192, 0.30205652, 0.28616927, 0.3337731 , 0.07249666,\n",
      "       0.00730804, 0.29148372, 0.19433004, 0.3125438 ]), 'std': array([0.07856543, 0.08016264, 0.07900642, 0.09663687, 0.08940903,\n",
      "       0.08163167, 0.08889727, 0.08184323, 0.09073389, 0.05365633,\n",
      "       0.01807999, 0.10828142, 0.10158471, 0.19213894])}\n",
      "Val split\n",
      "Checking for data files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35599it [00:00, 266608.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All pickle files were found!\n",
      "length labels: 168\n",
      "length pickle files: 168\n",
      "length local ids: 102\n",
      "{'mean': array([0.17615442, 0.15639622, 0.15478129, 0.16052005, 0.18210957,\n",
      "       0.25861192, 0.30205652, 0.28616927, 0.3337731 , 0.07249666,\n",
      "       0.00730804, 0.29148372, 0.19433004, 0.3125438 ]), 'std': array([0.07856543, 0.08016264, 0.07900642, 0.09663687, 0.08940903,\n",
      "       0.08163167, 0.08889727, 0.08184323, 0.09073389, 0.05365633,\n",
      "       0.01807999, 0.10828142, 0.10158471, 0.19213894])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset = dataset.train_val_split(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--weighted_loss_fn'], dest='weighted_loss_fn', nargs=0, const=True, default=False, type=None, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = ArgumentParser()\n",
    "parser.add_argument(\"--max_epochs\", type=int, default=1000)\n",
    "parser.add_argument(\"--patience\", type=int, default=10)\n",
    "parser.add_argument(\"--gpus\", type=int, default=0)\n",
    "parser.add_argument(\"--wandb\", default=False, action=\"store_true\")\n",
    "parser.add_argument(\"--weighted_loss_fn\", default=False, action=\"store_true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = STR2MODEL[\"land_cover\"].add_model_specific_args(parser).parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for data files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35599it [00:00, 92487.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All pickle files were found!\n",
      "length labels: 837\n",
      "length pickle files: 837\n",
      "length local ids: 490\n",
      "Found normalizing dict normalizing_dict_Nigeria_Cameroon_Benin_Togo_Ghana.pkl\n",
      "Loading normalizing dict.\n",
      "{'mean': array([0.17615442, 0.15639622, 0.15478129, 0.16052005, 0.18210957,\n",
      "       0.25861192, 0.30205652, 0.28616927, 0.3337731 , 0.07249666,\n",
      "       0.00730804, 0.29148372, 0.19433004, 0.3125438 ]), 'std': array([0.07856543, 0.08016264, 0.07900642, 0.09663687, 0.08940903,\n",
      "       0.08163167, 0.08889727, 0.08184323, 0.09073389, 0.05365633,\n",
      "       0.01807999, 0.10828142, 0.10158471, 0.19213894])}\n",
      "Train split\n",
      "Checking for data files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35599it [00:00, 124717.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All pickle files were found!\n",
      "length labels: 669\n",
      "length pickle files: 669\n",
      "length local ids: 388\n",
      "{'mean': array([0.17615442, 0.15639622, 0.15478129, 0.16052005, 0.18210957,\n",
      "       0.25861192, 0.30205652, 0.28616927, 0.3337731 , 0.07249666,\n",
      "       0.00730804, 0.29148372, 0.19433004, 0.3125438 ]), 'std': array([0.07856543, 0.08016264, 0.07900642, 0.09663687, 0.08940903,\n",
      "       0.08163167, 0.08889727, 0.08184323, 0.09073389, 0.05365633,\n",
      "       0.01807999, 0.10828142, 0.10158471, 0.19213894])}\n",
      "Val split\n",
      "Checking for data files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35599it [00:00, 183432.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All pickle files were found!\n",
      "length labels: 168\n",
      "length pickle files: 168\n",
      "length local ids: 102\n",
      "{'mean': array([0.17615442, 0.15639622, 0.15478129, 0.16052005, 0.18210957,\n",
      "       0.25861192, 0.30205652, 0.28616927, 0.3337731 , 0.07249666,\n",
      "       0.00730804, 0.29148372, 0.19433004, 0.3125438 ]), 'std': array([0.07856543, 0.08016264, 0.07900642, 0.09663687, 0.08940903,\n",
      "       0.08163167, 0.08889727, 0.08184323, 0.09073389, 0.05365633,\n",
      "       0.01807999, 0.10828142, 0.10158471, 0.19213894])}\n",
      "Number of model parameters: 28162\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Namespace(add_geowiki=True, add_togo=True, alpha=10, batch_size=64, data_folder='/home/gajo/code/togo-crop-mask/notebooks/../data', gpus=0, hidden_vector_size=64, learning_rate=0.001, lstm_dropout=0.2, max_epochs=1000, model_base='lstm', multi_headed=True, num_classification_layers=2, num_lstm_layers=1, patience=10, probability_threshold=0.5, remove_b1_b10=True, wandb=False, weighted_loss_fn=False)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = STR2MODEL[\"land_cover\"](model_args)\n",
    "model.hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_args_dict = vars(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET MODIFICATIONS TO DEFAULT MODEL ARGUMENTS:\n",
    "new_model_args_dict['add_togo'] = False\n",
    "new_model_args_dict['multi_headed'] = False\n",
    "new_model_args_dict['num_classification_layers'] = 1\n",
    "new_model_args_dict['max_epochs'] = 100 # Just for dev\n",
    "new_model_args_dict['weighted_loss_fn'] = True\n",
    "new_model_args_dict['hidden_vector_size'] = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for data files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35599it [00:00, 94942.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All pickle files were found!\n",
      "length labels: 837\n",
      "length pickle files: 837\n",
      "length local ids: 490\n",
      "Found normalizing dict normalizing_dict_Nigeria_Cameroon_Benin_Togo_Ghana.pkl\n",
      "Loading normalizing dict.\n",
      "{'mean': array([0.17615442, 0.15639622, 0.15478129, 0.16052005, 0.18210957,\n",
      "       0.25861192, 0.30205652, 0.28616927, 0.3337731 , 0.07249666,\n",
      "       0.00730804, 0.29148372, 0.19433004, 0.3125438 ]), 'std': array([0.07856543, 0.08016264, 0.07900642, 0.09663687, 0.08940903,\n",
      "       0.08163167, 0.08889727, 0.08184323, 0.09073389, 0.05365633,\n",
      "       0.01807999, 0.10828142, 0.10158471, 0.19213894])}\n",
      "Train split\n",
      "Checking for data files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35599it [00:00, 125406.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All pickle files were found!\n",
      "length labels: 669\n",
      "length pickle files: 669\n",
      "length local ids: 388\n",
      "{'mean': array([0.17615442, 0.15639622, 0.15478129, 0.16052005, 0.18210957,\n",
      "       0.25861192, 0.30205652, 0.28616927, 0.3337731 , 0.07249666,\n",
      "       0.00730804, 0.29148372, 0.19433004, 0.3125438 ]), 'std': array([0.07856543, 0.08016264, 0.07900642, 0.09663687, 0.08940903,\n",
      "       0.08163167, 0.08889727, 0.08184323, 0.09073389, 0.05365633,\n",
      "       0.01807999, 0.10828142, 0.10158471, 0.19213894])}\n",
      "Val split\n",
      "Checking for data files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35599it [00:00, 214960.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All pickle files were found!\n",
      "length labels: 168\n",
      "length pickle files: 168\n",
      "length local ids: 102\n",
      "{'mean': array([0.17615442, 0.15639622, 0.15478129, 0.16052005, 0.18210957,\n",
      "       0.25861192, 0.30205652, 0.28616927, 0.3337731 , 0.07249666,\n",
      "       0.00730804, 0.29148372, 0.19433004, 0.3125438 ]), 'std': array([0.07856543, 0.08016264, 0.07900642, 0.09663687, 0.08940903,\n",
      "       0.08163167, 0.08889727, 0.08184323, 0.09073389, 0.05365633,\n",
      "       0.01807999, 0.10828142, 0.10158471, 0.19213894])}\n",
      "Number of global labels: 837\n",
      "Number of local labels: 0\n",
      "Global class weights: tensor([1, 6])\n",
      "Local class weights: None\n",
      "Number of model parameters: 19777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Namespace(add_geowiki=True, add_togo=False, alpha=10, batch_size=64, data_folder='/home/gajo/code/togo-crop-mask/notebooks/../data', gpus=0, hidden_vector_size=64, learning_rate=0.001, lstm_dropout=0.2, max_epochs=100, model_base='lstm', multi_headed=False, num_classification_layers=1, num_lstm_layers=1, patience=10, probability_threshold=0.5, remove_b1_b10=True, wandb=False, weighted_loss_fn=True)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize model with new arguments\n",
    "new_model_args = Namespace(**new_model_args_dict)\n",
    "model = STR2MODEL[\"land_cover\"](new_model_args)\n",
    "model.hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model parameters:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19777"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Number of model parameters:')\n",
    "sum(param.numel() for param in model.parameters() if param.requires_grad_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>()>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train_dataloader = lambda: DataLoader(train_dataset, batch_size=model.hparams.batch_size, shuffle=True)\n",
    "model.train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>()>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.val_dataloader = lambda: DataLoader(val_dataset, batch_size=model.hparams.batch_size)\n",
    "model.val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of global labels: 837\n",
      "Number of local labels: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([1, 6]), None)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.global_class_weights, model.local_class_weights = model.get_class_weights()\n",
    "model.global_class_weights, model.local_class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.normalizing_dict = train_dataset.normalizing_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59e28e362dd3445da18ee2de54ef1290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[ 16 120]\n",
      " [  0  32]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8de1531f3ee0442680f05ed85bfcc927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f028b1abc9ee4ccfa1a3d3f1be826815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[69 67]\n",
      " [ 4 28]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f579950c87a140ab948758c425d7a5a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[83 53]\n",
      " [ 8 24]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b96005cc7a0e426a9e12512ba0ada534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[88 48]\n",
      " [ 9 23]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2638579c57234570a64c588a623a3f47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[88 48]\n",
      " [ 9 23]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e459777a1af419894cde02e0aef8802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[83 53]\n",
      " [ 8 24]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd60a4999097402da7851d51621c78b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[76 60]\n",
      " [ 5 27]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d64cb2dac2f405481aa2078bc4d6abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[77 59]\n",
      " [ 6 26]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb68f4aa8d994f298f0428e5d1be391d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[83 53]\n",
      " [ 8 24]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e45d44bac00f47d4beab940a61757eae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[77 59]\n",
      " [ 6 26]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2857ef3d7a44e5582aea8061fa91765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[75 61]\n",
      " [ 4 28]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8523b36f73474546b459493b23c00075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[76 60]\n",
      " [ 5 27]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb9ba1dfdea24ecf9ea7e476d8534ad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[81 55]\n",
      " [ 7 25]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb66fb1801334e799159b1c4e0709272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[75 61]\n",
      " [ 5 27]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6580602e19914596ab5098e54fbe4964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[75 61]\n",
      " [ 5 27]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "119a1c0966474ddaa659f6884022139e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[77 59]\n",
      " [ 5 27]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e5e597abd441e58273b38f8b4812bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[78 58]\n",
      " [ 6 26]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbbf211e350843ea9586a508547ed334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[76 60]\n",
      " [ 4 28]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deb2fafc72294aed9d4e5948c4a16cea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[74 62]\n",
      " [ 4 28]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a78505a8b414866b0a8d5e5d78af0b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[78 58]\n",
      " [ 6 26]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f845f62c039c49fc8e214b9c4dea2a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[79 57]\n",
      " [ 6 26]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57c2467a795d483894c52967f9616101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[81 55]\n",
      " [ 6 26]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8caf2d46c9854d9787f7844b1b651922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[73 63]\n",
      " [ 4 28]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a302897b65b4b74812927830978d188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[83 53]\n",
      " [ 7 25]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ad2eaee9f324bb49acf4cd7b9566f35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[77 59]\n",
      " [ 6 26]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4390f7f67cdb462485a4557db722146b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[77 59]\n",
      " [ 4 28]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f31529fe70540b493fe70f025d25cc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[84 52]\n",
      " [ 7 25]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bd0a23e5dee4c6a81e47040221b800a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[71 65]\n",
      " [ 2 30]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5b5f74422c44c6d81dcf5d348c56094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[82 54]\n",
      " [ 7 25]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2219264d95348a68f0d64e425ee1cee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[82 54]\n",
      " [ 4 28]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac16de0b9a61407891dfe1847dfb98ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[72 64]\n",
      " [ 5 27]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62d085fc35684763be53cca447c0b83d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[81 55]\n",
      " [ 7 25]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eed1a72e36134f909bb727c65c9fc2f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[73 63]\n",
      " [ 5 27]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d033d34b4bf43dbaa6de1e16f87106e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[83 53]\n",
      " [ 6 26]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0e1e25aac6a4900bbd306be3a63907c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[82 54]\n",
      " [ 8 24]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66a6a04af92e40f3b79e77f5b6c8ab52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[77 59]\n",
      " [ 3 29]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ecb9de6f3b40d695dfd572740b0462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[84 52]\n",
      " [ 8 24]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc994938d04342509968c149a75288e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[79 57]\n",
      " [ 2 30]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1da4663401d047668b0162e43baf5eac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[90 46]\n",
      " [10 22]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49951ea72af34a6cab583a7af401c9b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[79 57]\n",
      " [ 3 29]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c0ebd086784b49bdab3ce53a238e43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[84 52]\n",
      " [ 7 25]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4576a25041440d6aa8a5661f1d88457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[87 49]\n",
      " [ 8 24]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ddd247227148dea92fc1d52d261f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[86 50]\n",
      " [ 7 25]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae59bb4f1925467690f4e276a09f6f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[88 48]\n",
      " [ 9 23]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55112bf97a5a49cf8348e879c04bec43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[83 53]\n",
      " [ 4 28]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b30119ef401405e99576731d253f098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[82 54]\n",
      " [ 5 27]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bdbede20e694526b7d2368bf8923933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[90 46]\n",
      " [ 8 24]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47bc7f93fc5e4114b04fd9c59a4b6900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[86 50]\n",
      " [ 5 27]]\n"
     ]
    }
   ],
   "source": [
    "trainer = train_model(model, new_model_args) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating using the Nigeria evaluation dataset!\n",
      "Number of instances in nigeria_farmlands_v2 test set: 739\n",
      "{'mean': array([0.17615442, 0.15639622, 0.15478129, 0.16052005, 0.18210957,\n",
      "       0.25861192, 0.30205652, 0.28616927, 0.3337731 , 0.07249666,\n",
      "       0.00730804, 0.29148372, 0.19433004, 0.3125438 ]), 'std': array([0.07856543, 0.08016264, 0.07900642, 0.09663687, 0.08940903,\n",
      "       0.08163167, 0.08889727, 0.08184323, 0.09073389, 0.05365633,\n",
      "       0.01807999, 0.10828142, 0.10158471, 0.19213894])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a298b6a123840a3866cd665ab1dac07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[185 161]\n",
      " [268 125]]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TEST RESULTS\n",
      "{'test_loss': 8.46878719329834, 'test_roc_auc_score': 0.36391180926326316, 'test_precision_score': 0.4370629370629371, 'test_recall_score': 0.31806615776081426, 'test_f1_score': 0.36818851251840945, 'test_accuracy': 0.41948579161028415}\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DONE:\n",
    "- Support splitting dataset into train/val and update attributes (self.labels, self.pickle_files)\n",
    "    - Could either create dataset and split dataset, or split dataframe with sklearn and then create separate Geowiki datasets for each subset --> went for a hybrid\n",
    "    - Stratify:\n",
    "        - By country -> OK (just need to get rid of nans if there are any)\n",
    "        - By label (would need to define therhesold first)\n",
    "        - By both (https://stackoverflow.com/questions/45516424/sklearn-train-test-split-on-pandas-stratify-by-multiple-columns)\n",
    "- Add confusion matrix --> just printing it for now as tensorboard integration of lightning is only accepting scalars\n",
    "- Test multihead training with Nigeria (and Togo to see how much the local head helped in the original paper)\n",
    "    - Figure out normalizing dict for train and val set--> OK. just using normalizing_dict of full geowiki\n",
    "- Train of subset of countries.\n",
    "    - Need to figure out how to deal with normalizing dict automatically in this case (currently just loading the ones previously computed)\n",
    "    - Weighted loss fuction in multihead case. --> Probably need some separate class weights per head\n",
    "## TODOS:\n",
    "- Implement into src\n",
    "    - Pass countries_subset as list for argparser and prevent error of logging with tensorboard by not logging lists like confusion matrix or countries subset.\n",
    "    - Generate csv file with labels per country (in geowikiEngineer, and can be call in engineer.py script)\n",
    "    - Move all geowiki pickle files to all folder\n",
    "- Add only Africa\n",
    "- Later maybe see a way if I could just inherent from LandTypeClassificationDataset so I don't repeat too much code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('togo-paper')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13 |Anaconda, Inc.| (default, Jun  4 2021, 14:25:59) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f1d702b24b358fb38573032b2736288a41648cae2db041d7fdb41486d06c5511"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
