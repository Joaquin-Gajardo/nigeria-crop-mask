{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from argparse import ArgumentParser, Namespace\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import pickle\n",
    "from typing import Tuple, List, Dict, Union, Optional, TypeVar, Type\n",
    "from itertools import permutations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.engineer.geowiki import GeoWikiDataInstance, GeoWikiEngineer\n",
    "from src.exporters.sentinel.cloudfree import BANDS\n",
    "from src.models import STR2MODEL, train_model\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create custom Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "GeowikiDatasetType = TypeVar('GeowikiDatasetType', bound='Parent') # for typing\n",
    "\n",
    "class GeowikiDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_dir: Union[Path, str],\n",
    "                csv_file: str='geowiki_labels_country_crs4326.csv',\n",
    "                countries_subset: Optional[List[str]]=None,\n",
    "                countries_to_weight: Optional[List[str]]=None,\n",
    "                remove_b1_b10: bool=True,\n",
    "                labels: Optional[pd.DataFrame]=None # if this is passed csv_file will be ignored\n",
    "                ) -> None:\n",
    "        \n",
    "        # Constructor arguments\n",
    "        self.data_dir = data_dir\n",
    "        self.csv_file = csv_file\n",
    "        self.countries_subset = countries_subset\n",
    "        self.countries_to_weight = countries_to_weight\n",
    "        self.remove_b1_b10 = remove_b1_b10\n",
    "        \n",
    "        # Instance parameters\n",
    "        self.bands_to_remove = [\"B1\", \"B10\"]\n",
    "        self.crop_probability_threshold = 0.5\n",
    "\n",
    "        # Functions\n",
    "        if labels is None:\n",
    "            self.labels = pd.read_csv(self.data_dir / self.csv_file)\n",
    "            self.labels.loc[self.labels['country'].isnull(), 'country'] = 'unknown'\n",
    "            if self.countries_subset:\n",
    "                self.labels = self.labels[self.labels['country'].str.lower().isin(list(map(str.lower, self.countries_subset)))].reset_index(drop=True)\n",
    "        else:\n",
    "            self.labels = labels\n",
    "        self.pickle_files = self.get_pickle_files_paths(self.data_dir / 'all')\n",
    "        self.file_identifiers_countries_to_weight = self.get_file_ids_for_countries(self.countries_to_weight)\n",
    "        print('length labels:', len(self.labels))\n",
    "        print('length pickle files:', len(self.pickle_files))\n",
    "        print('length local ids:', len(self.file_identifiers_countries_to_weight))\n",
    "\n",
    "        # Normalizing dictionary\n",
    "        self.normalizing_dict = self.get_normalizing_dict()\n",
    "        #self.normalizing_dict = self.load_files_and_normalizing_dict(data_dir)[1]\n",
    "        print(self.normalizing_dict)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Return the data, label, and weight tensors.\n",
    "        \"\"\"\n",
    "        target_file = self.pickle_files[index]\n",
    "        identifier = int(target_file.name.split('_')[0])\n",
    "\n",
    "        with target_file.open(\"rb\") as f:\n",
    "            target_datainstance = pickle.load(f)\n",
    "\n",
    "        if isinstance(target_datainstance, GeoWikiDataInstance):\n",
    "            if self.crop_probability_threshold is None:\n",
    "                label = target_datainstance.crop_probability\n",
    "            else:\n",
    "                label = int(target_datainstance.crop_probability >= self.crop_probability_threshold)\n",
    "        else:\n",
    "            raise RuntimeError(f\"Unrecognized data instance type {type(target_datainstance)}\")\n",
    "\n",
    "        is_local = 0\n",
    "        if identifier in self.file_identifiers_countries_to_weight:\n",
    "           is_local = 1\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(\n",
    "                self.remove_bands(x=self._normalize(target_datainstance.labelled_array))\n",
    "            ).float(),\n",
    "            torch.tensor(label).float(),\n",
    "            torch.tensor(is_local).long(),\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def num_output_classes(self) -> int:\n",
    "        return 1\n",
    "\n",
    "    @property\n",
    "    def num_input_features(self) -> int:\n",
    "        # assumes the first value in the tuple is x\n",
    "        assert len(self.pickle_files) > 0, \"No files to load!\"\n",
    "        output_tuple = self[0]\n",
    "        return output_tuple[0].shape[1]\n",
    "\n",
    "    @property\n",
    "    def num_timesteps(self) -> int:\n",
    "        # assumes the first value in the tuple is x\n",
    "        assert len(self.pickle_files) > 0, \"No files to load!\"\n",
    "        output_tuple = self[0]\n",
    "        return output_tuple[0].shape[0]\n",
    "\n",
    "    def remove_bands(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Expects the input to be of shape [timesteps, bands]\n",
    "        \"\"\"\n",
    "        if self.remove_b1_b10:\n",
    "            indices_to_remove: List[int] = []\n",
    "            for band in self.bands_to_remove:\n",
    "                indices_to_remove.append(BANDS.index(band))\n",
    "\n",
    "            bands_index = 1 if len(x.shape) == 2 else 2\n",
    "            indices_to_keep = [i for i in range(x.shape[bands_index]) if i not in indices_to_remove]\n",
    "            if len(x.shape) == 2:\n",
    "                # timesteps, bands\n",
    "                return x[:, indices_to_keep]\n",
    "            else:\n",
    "                # batches, timesteps, bands\n",
    "                return x[:, :, indices_to_keep]\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def _normalize(self, array: np.ndarray) -> np.ndarray:\n",
    "        if self.normalizing_dict is None:\n",
    "            return array\n",
    "        else:\n",
    "            return (array - self.normalizing_dict[\"mean\"]) / self.normalizing_dict[\"std\"]\n",
    "\n",
    "    @staticmethod\n",
    "    def load_files_and_normalizing_dict(\n",
    "        features_dir: Path, subset_name: str='training', file_name: str=\"normalizing_dict.pkl\"\n",
    "    ) -> Tuple[List[Path], Optional[Dict[str, np.ndarray]]]:\n",
    "        pickle_files = list((features_dir / subset_name).glob(\"*.pkl\"))\n",
    "\n",
    "        # try loading the normalizing dict. By default, if it exists we will use it\n",
    "        if (features_dir / file_name).exists():\n",
    "            with (features_dir / file_name).open(\"rb\") as f:\n",
    "                normalizing_dict = pickle.load(f)\n",
    "        else:\n",
    "            normalizing_dict = None\n",
    "\n",
    "        return pickle_files, normalizing_dict\n",
    "    \n",
    "    def search_normalizing_dict(self, default_file_name: str=\"normalizing_dict.pkl\") -> Optional[Path]:\n",
    "        '''\n",
    "        Searches for the normalizing dict file in the self.data_dir directory and returns its path. Returns None if it was not found.\n",
    "        '''\n",
    "        prefix = default_file_name.split('.')[0]\n",
    "        \n",
    "        if not self.countries_subset:\n",
    "            file_path = self.data_dir / default_file_name\n",
    "            if file_path.exists():\n",
    "                print(f'Found normalizing dict {file_path.name}')\n",
    "                return file_path\n",
    "        elif len(self.countries_subset) == 1 and self.countries[0].lower() == 'africa':\n",
    "            raise NotImplementedError # TODO\n",
    "\n",
    "        else:\n",
    "            assert len(self.countries_subset) < 10, 'Execution time will be too big!' # TODO: add warning when passing subset to constructor\n",
    "            countries_permutations = list(permutations(self.countries_subset))\n",
    "            countries_permutations = ['_'.join(permutation) for permutation in countries_permutations]\n",
    "            for permutation in countries_permutations:\n",
    "                file_name = f\"{prefix}_{permutation}.pkl\"\n",
    "                file_path = self.data_dir / file_name\n",
    "                if file_path.exists():\n",
    "                    print(f'Found normalizing dict {file_name}')\n",
    "                    return file_path\n",
    "        print('Normalizing dict not found.')\n",
    "        return None\n",
    "    \n",
    "    def get_normalizing_dict(self, save: bool=False) -> Dict:\n",
    "        # Return dict if it was found or create it and save\n",
    "        default_file_name = \"normalizing_dict.pkl\"\n",
    "        file_path = self.search_normalizing_dict(default_file_name)\n",
    "        if file_path:\n",
    "            print('Loading normalizing dict.')\n",
    "            return self.load_files_and_normalizing_dict(self.data_dir, file_name=file_path.name)[1]\n",
    "        else:\n",
    "            print('Calculating normalizing dict...')\n",
    "            assert len(self) == len(self.pickle_files), 'Length of self.labels must be the same as of the list of pickle files.'\n",
    "            geowiki_engineer = GeoWikiEngineer(Path('../data'))\n",
    "            \n",
    "            for file_path in tqdm(self.pickle_files):\n",
    "                identifier = int(file_path.name.split('_')[0])\n",
    "                with file_path.open(\"rb\") as f:   \n",
    "                    target_datainstance = pickle.load(f)\n",
    "                geowiki_engineer.update_normalizing_values(target_datainstance.labelled_array)\n",
    "\n",
    "            normalizing_dict = geowiki_engineer.calculate_normalizing_dict()\n",
    "\n",
    "            # Write file\n",
    "            if save:    \n",
    "                if self.countries_subset:\n",
    "                    prefix = default_file_name.split('.')[0]\n",
    "                    countries_str = '_'.join(self.countries_subset)\n",
    "                    file_name = f\"{prefix}_{countries_str}.pkl\"\n",
    "                else:\n",
    "                    file_name = default_file_name\n",
    "                file_path = self.data_dir / file_name\n",
    "                print('Saving normalizing dict', file_path.name)\n",
    "                with file_path.open(\"wb\") as f:\n",
    "                    pickle.dump(normalizing_dict, f)\n",
    "\n",
    "            return normalizing_dict\n",
    "            \n",
    "    def get_pickle_files_paths(self, folder_path: Path) -> Tuple[List[Path]]:\n",
    "        file_paths = self.labels.filename.tolist()\n",
    "        print('Checking for data files')\n",
    "        pickle_files = [path for path in tqdm(folder_path.glob('*.pkl')) if path.name in file_paths]\n",
    "        self._check_label_files(pickle_files)\n",
    "        return pickle_files\n",
    "\n",
    "    def _check_label_files(self, pickle_files) -> None:\n",
    "        same_files = set([file.name for file in pickle_files]) == set(self.labels.filename.tolist())\n",
    "        assert same_files, \"Some pickle files of the labels were not found!\"\n",
    "        print('All pickle files were found!')\n",
    "\n",
    "    def get_file_ids_for_countries(self, countries_list: List[str]) -> List[int]:\n",
    "        file_ids = []\n",
    "        if countries_list:\n",
    "            countries_list_lowercase = list(map(str.lower, countries_list))\n",
    "            file_ids.extend(self.labels[self.labels['country'].str.lower().isin(countries_list_lowercase)]['identifier'].tolist())\n",
    "        return file_ids\n",
    "\n",
    "    @classmethod\n",
    "    def train_val_split(cls: Type[GeowikiDatasetType], class_instance: Type[GeowikiDatasetType], train_size: float=0.8, stratify_column: Optional[str]=None) -> Tuple[GeowikiDatasetType]:\n",
    "        # Split labels dataframe\n",
    "        stratify = None if not stratify_column else class_instance.labels[stratify_column]\n",
    "        df_train, df_val = train_test_split(class_instance.labels, train_size=train_size, stratify=stratify, random_state=42)\n",
    "        df_train.reset_index(drop=True, inplace=True) \n",
    "        df_val.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # Create two new GeowikiDataset instances (train and val)\n",
    "        print('Train split')\n",
    "        train_dataset = cls(class_instance.data_dir, countries_to_weight=class_instance.countries_to_weight, labels=df_train)\n",
    "        print('Val split')\n",
    "        val_dataset = cls(class_instance.data_dir, countries_to_weight=class_instance.countries_to_weight, labels=df_val)\n",
    "        return train_dataset, val_dataset\n",
    "\n",
    "    def get_file_by_identifier(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for data files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35599it [00:08, 4383.52it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All pickle files were found!\n",
      "length labels: 35599\n",
      "length pickle files: 35599\n",
      "length local ids: 490\n",
      "Found normalizing dict normalizing_dict.pkl\n",
      "Loading normalizing dict.\n",
      "{'mean': array([0.19353804, 0.17112217, 0.16083624, 0.16354993, 0.18635676,\n",
      "       0.25554994, 0.29061711, 0.28009877, 0.31469831, 0.10141977,\n",
      "       0.0087153 , 0.22964706, 0.15255525, 0.3221835 ]), 'std': array([0.14932182, 0.15265479, 0.14360899, 0.16329558, 0.15796025,\n",
      "       0.14746618, 0.15011357, 0.14306833, 0.14913972, 0.09338568,\n",
      "       0.02771975, 0.1111936 , 0.09549155, 0.23958353])}\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path('../data/features/geowiki_landcover_2017')\n",
    "subset = ['Ghana', 'Togo', 'Nigeria', 'Cameroon', 'Benin'] #['Nigeria'] #None\n",
    "subset = ['Nigeria', 'India']\n",
    "subset = None\n",
    "#subset = ['Ghana', 'Togo', 'Nigeria', 'Chad', 'Democratic Republic of the Congo', 'Ethiopia', 'Chad', 'Mali']\n",
    "dataset = GeowikiDataset(data_dir, 'geowiki_labels_country_crs4326.csv', countries_subset=subset, countries_to_weight=['Nigeria'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "China                       3923\n",
       "United States of America    3489\n",
       "Brazil                      3165\n",
       "Russia                      2511\n",
       "India                       1589\n",
       "                            ... \n",
       "Northern Cyprus                1\n",
       "Samoa                          1\n",
       "Saint Lucia                    1\n",
       "Trinidad and Tobago            1\n",
       "Mauritius                      1\n",
       "Name: country, Length: 177, dtype: int64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.labels['country'].value_counts()#.to_csv('geowiki_points_per_country.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>featurecla</th>\n",
       "      <th>scalerank</th>\n",
       "      <th>LABELRANK</th>\n",
       "      <th>SOVEREIGNT</th>\n",
       "      <th>SOV_A3</th>\n",
       "      <th>ADM0_DIF</th>\n",
       "      <th>LEVEL</th>\n",
       "      <th>TYPE</th>\n",
       "      <th>ADMIN</th>\n",
       "      <th>ADM0_A3</th>\n",
       "      <th>...</th>\n",
       "      <th>FCLASS_TR</th>\n",
       "      <th>FCLASS_ID</th>\n",
       "      <th>FCLASS_PL</th>\n",
       "      <th>FCLASS_GR</th>\n",
       "      <th>FCLASS_IT</th>\n",
       "      <th>FCLASS_NL</th>\n",
       "      <th>FCLASS_SE</th>\n",
       "      <th>FCLASS_BD</th>\n",
       "      <th>FCLASS_UA</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Admin-0 country</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Sovereign country</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((31.28789 -22.40205, 31.19727 -22.344...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Admin-0 country</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Zambia</td>\n",
       "      <td>ZMB</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Sovereign country</td>\n",
       "      <td>Zambia</td>\n",
       "      <td>ZMB</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((30.39609 -15.64307, 30.25068 -15.643...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Admin-0 country</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Yemen</td>\n",
       "      <td>YEM</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Sovereign country</td>\n",
       "      <td>Yemen</td>\n",
       "      <td>YEM</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>MULTIPOLYGON (((53.08564 16.64839, 52.58145 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Admin-0 country</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Vietnam</td>\n",
       "      <td>VNM</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Sovereign country</td>\n",
       "      <td>Vietnam</td>\n",
       "      <td>VNM</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>MULTIPOLYGON (((104.06396 10.39082, 104.08301 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Admin-0 country</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Venezuela</td>\n",
       "      <td>VEN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Sovereign country</td>\n",
       "      <td>Venezuela</td>\n",
       "      <td>VEN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>MULTIPOLYGON (((-60.82119 9.13838, -60.94141 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>Admin-0 country</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Sovereign country</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((66.52227 37.34849, 66.82773 37.37129...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>Admin-0 country</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Kashmir</td>\n",
       "      <td>KAS</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Indeterminate</td>\n",
       "      <td>Siachen Glacier</td>\n",
       "      <td>KAS</td>\n",
       "      <td>...</td>\n",
       "      <td>Unrecognized</td>\n",
       "      <td>Unrecognized</td>\n",
       "      <td>Unrecognized</td>\n",
       "      <td>Unrecognized</td>\n",
       "      <td>Unrecognized</td>\n",
       "      <td>Unrecognized</td>\n",
       "      <td>Unrecognized</td>\n",
       "      <td>Unrecognized</td>\n",
       "      <td>Unrecognized</td>\n",
       "      <td>POLYGON ((77.04863 35.10991, 77.00449 35.19634...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>Admin-0 country</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>ATA</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Indeterminate</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>ATA</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>MULTIPOLYGON (((-45.71777 -60.52090, -45.49971...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>Admin-0 country</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>NL1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Country</td>\n",
       "      <td>Sint Maarten</td>\n",
       "      <td>SXM</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((-63.12305 18.06895, -63.01118 18.068...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>Admin-0 country</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>Tuvalu</td>\n",
       "      <td>TUV</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Sovereign country</td>\n",
       "      <td>Tuvalu</td>\n",
       "      <td>TUV</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((179.21367 -8.52422, 179.20059 -8.534...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>242 rows × 162 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          featurecla  scalerank  LABELRANK   SOVEREIGNT SOV_A3  ADM0_DIF  \\\n",
       "0    Admin-0 country          1          3     Zimbabwe    ZWE         0   \n",
       "1    Admin-0 country          1          3       Zambia    ZMB         0   \n",
       "2    Admin-0 country          1          3        Yemen    YEM         0   \n",
       "3    Admin-0 country          3          2      Vietnam    VNM         0   \n",
       "4    Admin-0 country          5          3    Venezuela    VEN         0   \n",
       "..               ...        ...        ...          ...    ...       ...   \n",
       "237  Admin-0 country          1          3  Afghanistan    AFG         0   \n",
       "238  Admin-0 country          1          5      Kashmir    KAS         0   \n",
       "239  Admin-0 country          3          4   Antarctica    ATA         0   \n",
       "240  Admin-0 country          3          6  Netherlands    NL1         1   \n",
       "241  Admin-0 country          5          6       Tuvalu    TUV         0   \n",
       "\n",
       "     LEVEL               TYPE            ADMIN ADM0_A3  ...     FCLASS_TR  \\\n",
       "0        2  Sovereign country         Zimbabwe     ZWE  ...          None   \n",
       "1        2  Sovereign country           Zambia     ZMB  ...          None   \n",
       "2        2  Sovereign country            Yemen     YEM  ...          None   \n",
       "3        2  Sovereign country          Vietnam     VNM  ...          None   \n",
       "4        2  Sovereign country        Venezuela     VEN  ...          None   \n",
       "..     ...                ...              ...     ...  ...           ...   \n",
       "237      2  Sovereign country      Afghanistan     AFG  ...          None   \n",
       "238      2      Indeterminate  Siachen Glacier     KAS  ...  Unrecognized   \n",
       "239      2      Indeterminate       Antarctica     ATA  ...          None   \n",
       "240      2            Country     Sint Maarten     SXM  ...          None   \n",
       "241      2  Sovereign country           Tuvalu     TUV  ...          None   \n",
       "\n",
       "        FCLASS_ID     FCLASS_PL     FCLASS_GR     FCLASS_IT     FCLASS_NL  \\\n",
       "0            None          None          None          None          None   \n",
       "1            None          None          None          None          None   \n",
       "2            None          None          None          None          None   \n",
       "3            None          None          None          None          None   \n",
       "4            None          None          None          None          None   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "237          None          None          None          None          None   \n",
       "238  Unrecognized  Unrecognized  Unrecognized  Unrecognized  Unrecognized   \n",
       "239          None          None          None          None          None   \n",
       "240          None          None          None          None          None   \n",
       "241          None          None          None          None          None   \n",
       "\n",
       "        FCLASS_SE     FCLASS_BD     FCLASS_UA  \\\n",
       "0            None          None          None   \n",
       "1            None          None          None   \n",
       "2            None          None          None   \n",
       "3            None          None          None   \n",
       "4            None          None          None   \n",
       "..            ...           ...           ...   \n",
       "237          None          None          None   \n",
       "238  Unrecognized  Unrecognized  Unrecognized   \n",
       "239          None          None          None   \n",
       "240          None          None          None   \n",
       "241          None          None          None   \n",
       "\n",
       "                                              geometry  \n",
       "0    POLYGON ((31.28789 -22.40205, 31.19727 -22.344...  \n",
       "1    POLYGON ((30.39609 -15.64307, 30.25068 -15.643...  \n",
       "2    MULTIPOLYGON (((53.08564 16.64839, 52.58145 16...  \n",
       "3    MULTIPOLYGON (((104.06396 10.39082, 104.08301 ...  \n",
       "4    MULTIPOLYGON (((-60.82119 9.13838, -60.94141 9...  \n",
       "..                                                 ...  \n",
       "237  POLYGON ((66.52227 37.34849, 66.82773 37.37129...  \n",
       "238  POLYGON ((77.04863 35.10991, 77.00449 35.19634...  \n",
       "239  MULTIPOLYGON (((-45.71777 -60.52090, -45.49971...  \n",
       "240  POLYGON ((-63.12305 18.06895, -63.01118 18.068...  \n",
       "241  POLYGON ((179.21367 -8.52422, 179.20059 -8.534...  \n",
       "\n",
       "[242 rows x 162 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "world_map = gpd.read_file('../assets/ne_50m_admin_0_countries/ne_50m_admin_0_countries.shp')\n",
    "world_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to get splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt by splitting Geowiki dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratio = 0.2\n",
    "test_size = math.floor(len(dataset) * test_ratio)\n",
    "lenghts = [len(dataset) - test_size, test_size]\n",
    "lenghts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(lenghts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = random_split(dataset, lenghts)  #generator is not yet available in this pytorch version, generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.pickle_files # Subset class doesn't inheret properties like pickles files plus I would need to subset those as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With sklearn train test split on dataframe with labels and then create separate Geowiki datasets for each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset.labels\n",
    "df_subset = df[df['country'].isin(['Nigeria', 'Ghana'])]\n",
    "df_subset.groupby('country').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df_subset, test_size=0.1, stratify=df_subset['country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.groupby('country').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.groupby('country').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df_subset, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.groupby('country').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.groupby('country').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['country'].value_counts().to_csv('geowiki_points_per_country.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['country'].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = dataset.train_val_split(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ArgumentParser()\n",
    "parser.add_argument(\"--max_epochs\", type=int, default=1000)\n",
    "parser.add_argument(\"--patience\", type=int, default=10)\n",
    "parser.add_argument(\"--gpus\", type=int, default=0)\n",
    "parser.add_argument(\"--wandb\", default=False, action=\"store_true\")\n",
    "parser.add_argument(\"--weighted_loss_fn\", default=False, action=\"store_true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = STR2MODEL[\"land_cover\"].add_model_specific_args(parser).parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = STR2MODEL[\"land_cover\"](model_args)\n",
    "model.hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_args_dict = vars(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET MODIFICATIONS TO DEFAULT MODEL ARGUMENTS:\n",
    "new_model_args_dict['add_togo'] = False\n",
    "new_model_args_dict['multi_headed'] = False\n",
    "new_model_args_dict['num_classification_layers'] = 1\n",
    "new_model_args_dict['max_epochs'] = 100 # Just for dev\n",
    "new_model_args_dict['weighted_loss_fn'] = True\n",
    "new_model_args_dict['hidden_vector_size'] = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model with new arguments\n",
    "new_model_args = Namespace(**new_model_args_dict)\n",
    "model = STR2MODEL[\"land_cover\"](new_model_args)\n",
    "model.hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of model parameters:')\n",
    "sum(param.numel() for param in model.parameters() if param.requires_grad_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_dataloader = lambda: DataLoader(train_dataset, batch_size=model.hparams.batch_size)\n",
    "model.train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.val_dataloader = lambda: DataLoader(val_dataset, batch_size=model.hparams.batch_size)\n",
    "model.val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.global_class_weights, model.local_class_weights = model.get_class_weights([train_dataset, val_dataset])\n",
    "model.global_class_weights, model.local_class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = train_model(model, new_model_args) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DONE:\n",
    "- Support splitting dataset into train/val and update attributes (self.labels, self.pickle_files)\n",
    "    - Could either create dataset and split dataset, or split dataframe with sklearn and then create separate Geowiki datasets for each subset --> went for a hybrid\n",
    "    - Stratify:\n",
    "        - By country -> OK (just need to get rid of nans if there are any)\n",
    "        - By label (would need to define therhesold first)\n",
    "        - By both (https://stackoverflow.com/questions/45516424/sklearn-train-test-split-on-pandas-stratify-by-multiple-columns)\n",
    "- Add confusion matrix --> just printing it for now as tensorboard integration of lightning is only accepting scalars\n",
    "- Test multihead training with Nigeria (and Togo to see how much the local head helped in the original paper)\n",
    "    - Figure out normalizing dict for train and val set--> OK. just using normalizing_dict of full geowiki\n",
    "- Train of subset of countries.\n",
    "    - Need to figure out how to deal with normalizing dict automatically in this case (currently just loading the ones previously computed)\n",
    "    - Weighted loss fuction in multihead case. --> Probably need some separate class weights per head\n",
    "## TODOS:\n",
    "- Implement into src\n",
    "- Add only Africa\n",
    "- Later maybe see a way if I could just inherent from LandTypeClassificationDataset so I don't repeat too much code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('togo-paper')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13 |Anaconda, Inc.| (default, Jun  4 2021, 14:25:59) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f1d702b24b358fb38573032b2736288a41648cae2db041d7fdb41486d06c5511"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
