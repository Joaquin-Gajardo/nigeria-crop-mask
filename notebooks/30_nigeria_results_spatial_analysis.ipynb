{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from src.models import LandCoverMapper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load labels with coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(Path('../data/features/nigeria-cropharvest/labels.geojson'))\n",
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing arrays\n",
    "arrays_folder = Path('../data/features/nigeria-cropharvest/features/arrays')\n",
    "existing_arrays_ids = sorted([int(str(path.stem).split('_')[0]) for path in arrays_folder.glob('*.h5')])\n",
    "missing_files_identifiers = list(set(list(range(1827))) - set(existing_arrays_ids))\n",
    "missing_files_identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set = 'testing' # 'validation'\n",
    "val_gdf = gdf[gdf['new_set'] == eval_set]\n",
    "\n",
    "# We don't conside the missing h5 files to ensure a fair comparison with the other models\n",
    "val_gdf = val_gdf.loc[~val_gdf['identifier'].isin(missing_files_identifiers)]\n",
    "val_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = Path('../data/lightning_logs/version_949/testing')\n",
    "preds = np.load(results_path / 'all_preds.npy')\n",
    "labels = np.load(results_path / 'all_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, labels # don't have the coordinates though like this though, find out if order from gdf is the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODOS:\n",
    "- [x] Get test set predictions of the model (with saved predictions?)\n",
    "- [x] Check with validation set\n",
    "- [ ] Check if it they all give same results: saved model predictions, predicting again with the saved model in the test set, and querying the map on the test set points\n",
    "- [ ] Check neighbours1 model without weighted loss function (has a more balanced recall and precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that all labels are in the same order as the dataframe with labels so predictions can be appended as a column\n",
    "assert all(labels.squeeze() == val_gdf.is_crop.to_numpy()), 'labels saved from model checkpoint are not in the same order as the dataframe with labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_gdf['preds'] = preds.squeeze()\n",
    "val_gdf['preds_thr0.5'] = (val_gdf['preds'] > 0.5).astype(int)    \n",
    "val_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_gdf.plot(column='preds', legend=True, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_gdf.plot(column='preds_thr0.5', legend=True, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_gdf.plot(column='is_crop', legend=True, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same accuracy as reported in the results csv file: `results/final/lstm/results_final_lstm.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((val_gdf['preds'] > 0.5) == val_gdf['is_crop']).sum() / len(val_gdf) # should be 0.841758241758242 so it matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model from checkpoint and get predictions on test set (to compare with saved ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../data/lightning_logs/version_867/checkpoints/.ckpt\"\n",
    "model_path = str(results_path.parent / 'checkpoints' / 'epoch=22.ckpt')\n",
    "\n",
    "print(f\"Using model {model_path}\")\n",
    "inference = True if eval_set == 'testing' else False\n",
    "model = LandCoverMapper.load_from_checkpoint(model_path, inference=inference)\n",
    "#model.hparams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer()\n",
    "trainer.test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer()\n",
    "trainer.test(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nigeria-crop-mask-gpu3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
